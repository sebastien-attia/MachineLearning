{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet on Neural Networks 2\n",
    "\n",
    "This cheat sheet is an article on Neural Networks and follows [the first sheet cheat on Neural Networks](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a). The purpose of this article is to make alive the equations we have described previously.\n",
    "\n",
    "\n",
    "## Definition of the cost and activation functions\n",
    "\n",
    "\n",
    "### The cost function\n",
    "\n",
    "Remember that the cost function $J(\\theta)$ is defined as follow:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[ cost( a^{(L)}_k, y^{(i)}_k) \\right] + \\frac{\\lambda}{2m}\\sum_{l=2}^{L} \\sum_{j=1}^{s_l} \\sum_{i=1}^{s_{l+1}} ( \\theta_{i,j}^{(l)})^2\n",
    "$$\n",
    "\n",
    "The cost function $cost( a^{(L)}_k, y^{(i)}_k)$ can be defined as in the linear regression or as in logistic regression:\n",
    "\n",
    "\n",
    "|  | $cost( a^{(L)}_k, y^{(i)}_k)$ | $\\frac{\\partial cost( a^{(L)}_k, y^{(i)}_k)}{\\partial a^{(L)}_k} $ |\n",
    "|:---------------------:|:-------------------------------:|------------------------------------------------|\n",
    "| Linear regression   | $1/2.(a^{(L)}_k-y^{(i)}_k)^2$ | $(a^{(L)}_k-y^{(i)}_k)$   |\n",
    "| Logistic regression | $y^{(i)}_k log(a^{(L)}_k) + (1 - y^{(i)}_k).log(1 - a^{(L)}_k)$ | $\\frac{y^{(i)}_k}{a^{(L)}_k} - \\frac{1 - y^{(i)}_k}{1 - a^{(L)}_k} $ |          |\n",
    "\n",
    "\n",
    "### The activation functions\n",
    "\n",
    "We consider the sigmoid and the hyperbolic tangent:\n",
    "\n",
    "\n",
    "| Activation function | g(z) | g'(z) |\n",
    "|---------------------|:------:|:-------:|\n",
    "| Sigmoid             | $\\frac{1}{1+e^{-z}}$ | g(z).(1-g(z)) |\n",
    "| Hyperbolic tangent  | $\\frac{e^z-e^{-z}}{e^{z} + e^{-z}}$  | $ 1 \\text{-} (g(z))^2 $ |\n",
    "\n",
    "\n",
    "## Make the equations alive\n",
    "\n",
    "For the concrete implementation of the backpropagation algorithm, we will choose:\n",
    "- the cost function of the logistic regression and\n",
    "- the sigmoid for the activation function\n",
    "\n",
    "\n",
    "### Equations \n",
    "\n",
    "Let's express the __[third equation](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a#mjx-eq-3)__ of the previous article [\"Cheat sheet on Neural Networks 1\"](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(L)} & = \\nabla_{a^{(L)}} J(\\theta) \\odot g'(z^L)\\\\\n",
    "\\delta^{(L)}_p & = \\frac{\\partial J(\\theta)}{\\partial a^{(L)}_p} . g'(z^L_p),\\ \\forall p \\in {1, ..., s_L}\\\\\n",
    "\\delta^{(L)}_p & = - \\frac{\\partial \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[ cost( a^{(L)}_k, y^{(i)}_k) \\right] +{\\frac{\\lambda}{2m}\\sum_{l=2}^{L} \\sum_{j=1}^{s_l} \\sum_{i=1}^{s_{l+1}} ( \\theta_{i,j}^{(l)})^2}}{\\partial a^{(L)}_p} . g'(z^L_p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The second term is 0, because the regularization term is not dependent of $a^{(L)}_p$.  \n",
    "The first term is not equal to 0 in the sum, only when k=i and can then be reduced to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(L)}_p & = - \\frac{1}{m} \\frac{\\partial \\sum_{i=1}^m \\left[ cost( a^{(L)}_i, y^{(i)}_i) \\right]} {\\partial a^{(L)}_p} . g'(z^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial \\left[ cost( a^{(L)}_i, y^{(i)}_i) \\right]} {\\partial a^{(L)}_p} . g'(z^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{y^{(i)}_p}{a^{(L)}_p} - \\frac{1 - y^{(i)}_p}{1 - a^{(L)}_p} \\right) . g(z^L_p).(1-g(z^L_p))\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{y^{(i)}_p}{a^{(L)}_p} - \\frac{1 - y^{(i)}_p}{1 - a^{(L)}_p} \\right) . a^L_p.(1-a^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)}_p - a^{(L)}_p \\right)\\\\\n",
    "\\delta^{(L)} & = - \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} - a^{(L)} \\right)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The __[second eqution](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a#mjx-eq-2)__ becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(l)} & = [(\\theta^{(l+1)})^T . \\delta^{(l+1)}] \\odot g'(z^l)\\\\\n",
    "\\delta^{(l)} & = [(\\theta^{(l+1)})^T . \\delta^{(l+1)}] \\odot a^{(L)} \\odot (1-a^{(L)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Implementation of the backpropagation algorithm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> To Victor, Oscar and all those who will follow </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
