{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet on Neural Networks 2\n",
    "\n",
    "This cheat sheet is an article on Neural Networks and follows [the first sheet cheat on Neural Networks](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a). The purpose of this article is to make alive the equations we have described previously.\n",
    "\n",
    "\n",
    "## Definition of the cost and activation functions\n",
    "\n",
    "\n",
    "### The cost function\n",
    "\n",
    "Remember that the cost function $J(\\theta)$ is defined as follow:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[ cost( a^{(L)}_k, y^{(i)}_k) \\right] + \\frac{\\lambda}{2m}\\sum_{l=2}^{L} \\sum_{j=1}^{s_l} \\sum_{i=1}^{s_{l+1}} ( \\theta_{i,j}^{(l)})^2\n",
    "$$\n",
    "\n",
    "The cost function $cost( a^{(L)}_k, y^{(i)}_k)$ can be defined as in the linear regression or as in logistic regression:\n",
    "\n",
    "\n",
    "|  | $cost( a^{(L)}_k, y^{(i)}_k)$ | $\\frac{\\partial cost( a^{(L)}_k, y^{(i)}_k)}{\\partial a^{(L)}_k} $ |\n",
    "|:---------------------:|:-------------------------------:|------------------------------------------------|\n",
    "| Linear regression   | $1/2.(a^{(L)}_k-y^{(i)}_k)^2$ | $(a^{(L)}_k-y^{(i)}_k)$   |\n",
    "| Logistic regression | $y^{(i)}_k log(a^{(L)}_k) + (1 - y^{(i)}_k).log(1 - a^{(L)}_k)$ | $\\frac{y^{(i)}_k}{a^{(L)}_k} - \\frac{1 - y^{(i)}_k}{1 - a^{(L)}_k} $ |          |\n",
    "\n",
    "\n",
    "### The activation functions\n",
    "\n",
    "We consider the sigmoid and the hyperbolic tangent:\n",
    "\n",
    "\n",
    "| Activation function | g(z) | g'(z) |\n",
    "|---------------------|:------:|:-------:|\n",
    "| Sigmoid             | $\\frac{1}{1+e^{-z}}$ | g(z).(1-g(z)) |\n",
    "| Hyperbolic tangent  | $\\frac{e^z-e^{-z}}{e^{z} + e^{-z}}$  | $ 1 \\text{-} (g(z))^2 $ |\n",
    "\n",
    "\n",
    "## Make the equations alive\n",
    "\n",
    "For the concrete implementation of the backpropagation algorithm, we will choose:\n",
    "- the cost function of the __logistic regression__ and\n",
    "- the __sigmoid__ for the activation function\n",
    "\n",
    "\n",
    "### Equations \n",
    "\n",
    "Let's express the __[third equation](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a#mjx-eq-3)__ of the previous article [\"Cheat sheet on Neural Networks 1\"](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(L)} & = \\nabla_{a^{(L)}} J(\\theta) \\odot g'(z^L)\\\\\n",
    "\\delta^{(L)}_p & = \\frac{\\partial J(\\theta)}{\\partial a^{(L)}_p} . g'(z^L_p),\\ \\forall p \\in {1, ..., s_L}\\\\\n",
    "\\delta^{(L)}_p & = - \\frac{\\partial \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[ cost( a^{(L)}_k, y^{(i)}_k) \\right] +{\\frac{\\lambda}{2m}\\sum_{l=2}^{L} \\sum_{j=1}^{s_l} \\sum_{i=1}^{s_{l+1}} ( \\theta_{i,j}^{(l)})^2}}{\\partial a^{(L)}_p} . g'(z^L_p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The second term is 0, because the regularization term is not dependent of $a^{(L)}_p$.  \n",
    "The first term is not equal to 0 in the sum, only when k=p and can then be reduced to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(L)}_p & = - \\frac{1}{m} \\frac{\\partial \\sum_{i=1}^m \\left[ cost( a^{(L)}_i, y^{(i)}_i) \\right]} {\\partial a^{(L)}_p} . g'(z^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial \\left[ cost( a^{(L)}_i, y^{(i)}_i) \\right]} {\\partial a^{(L)}_p} . g'(z^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{y^{(i)}_p}{a^{(L)}_p} - \\frac{1 - y^{(i)}_p}{1 - a^{(L)}_p} \\right) . g(z^L_p).(1-g(z^L_p))\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{y^{(i)}_p}{a^{(L)}_p} - \\frac{1 - y^{(i)}_p}{1 - a^{(L)}_p} \\right) . a^L_p.(1-a^L_p)\\\\\n",
    "& = - \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)}_p - a^{(L)}_p \\right)\\\\\n",
    "\\delta^{(L)} & = - \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} - a^{(L)} \\right)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The __[second equation](https://blog.innovea.tech/https-medium-com-sebastien-attia-cheat-sheet-on-neural-networks-1-30735616584a#mjx-eq-2)__ becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(l)} & = [(\\theta^{(l+1)})^T . \\delta^{(l+1)}] \\odot g'(z^l)\\\\\n",
    "\\delta^{(l)} & = [(\\theta^{(l+1)})^T . \\delta^{(l+1)}] \\odot a^{(l)} \\odot (1-a^{(l)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### Implementation of the backpropagation algorithm\n",
    "\n",
    "The backpropagation algorithm has several flavours:\n",
    "- the batch gradient descent,\n",
    "- the stochastic gradient descent,\n",
    "- the mini-batch gradient descent,\n",
    "\n",
    "We define first the mini-batch gradient descent.  \n",
    "\n",
    "\n",
    "#### The mini-batch gradient descent\n",
    "\n",
    "We define the variable b, the mini-batch size.\n",
    "\n",
    "1. Randomly initialize the weights of the Neural Network, \n",
    "2. Split the training set $\\{(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})\\}$ in mini-batch of size b,\n",
    "3. For each mini-batch:  \n",
    "    0. initialize to 0 the accumulator $\\Delta^{(l)},\\ \\forall l \\in {2, ..., L}$\n",
    "    1. for each input of the mini-batch:  \n",
    "        1. perform the forward propagation, by applying recursevely the equation (0) on the input vector,  \n",
    "        2. Recursively, for each $l \\in {L, L-1, ..., 2}$:\n",
    "            1. compute the value of $\\delta^{(l)}$, when l=L, use equation (3), else use equation (2),\n",
    "            2. compute $\\Delta^{(l)} := \\Delta^{(l)} + \\nabla_{\\theta^{(l)}} J(\\theta)$ by applying equation 1\n",
    "            3. compute $\\Delta^{(l)} := \\Delta^{(l)} / (size\\ of\\ mini-batch),\\ \\forall l \\in {2, ..., L}$, (the last batch may have a size â‰  b),\n",
    "    3. optionally: compute an approximation of $\\nabla_{\\theta^{(l)}}$ (__gradient checking__) and compare the value to $\\Delta^{(l)}$\n",
    "    4. recompute the weights $\\theta^{(l)}$ of the Neural Network by applying: $\\theta^{(l)} := \\theta^{(l)} - \\alpha. \\Delta^{(l)},\\ \\forall l \\in {2, ..., L} $\n",
    "\n",
    "\n",
    "#### The batch gradient descent\n",
    "... is a mini-batch gradient descent with b = m.\n",
    "\n",
    "#### The stochastic gradient descent\n",
    "... is a mini-batch gradient descent with b = 1.\n",
    "\n",
    "\n",
    "## Implementation of the Neural Network in Python\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> To Victor, Oscar and all those who will follow </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
