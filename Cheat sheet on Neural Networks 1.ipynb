{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet on Neural Networks 1  \n",
    "\n",
    "The following article is a cheat sheet on neural networks. My sources are based on the following course and article:\n",
    "- the excellent [Machine Learning course](https://www.coursera.org/learn/machine-learning) on Coursera from Professor Andrew Ng, Stanford,\n",
    "- the very good [article from Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html), explaining the backpropagation algorithm.\n",
    "\n",
    "\n",
    "## Why the neural networks are powerful ?\n",
    "\n",
    "It is proven mathematically that:  \n",
    "\n",
    "> Suppose we‚Äôre given a [continuous] function f(x) which we‚Äôd like to compute to within some desired accuracy œµ>0. The guarantee is that by using enough hidden neurons we can always find a neural network whose output g(x) satisfies:\n",
    "|g(x)‚àíf(x)|<œµ, for all inputs x.  \n",
    "\n",
    "_Michael Nielsen‚Ää‚Äî‚ÄäFrom the following [article](http://neuralnetworksanddeeplearning.com/chap4.html)_\n",
    "\n",
    "##  Conventions  \n",
    "Let‚Äôs define a neural network with the following convention:\n",
    "\n",
    "L = total number of layers in the network.  \n",
    "$s_l$ = number of units (not counting bias unit) in layer l.  \n",
    "K = number of units in output layer ( = $s_l$ ).  \n",
    "\n",
    "<img src=\"images/Neural_Network_definition.png\" />\n",
    "\n",
    "We define the matrix Œ∏ of the weights for the layer l as following:\n",
    "\n",
    "$$\n",
    "\\theta^{(l)} \\in \\mathbb{R}^{s_l \\times (s_{(l-1)}+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta^{(l)} = \n",
    "\\begin{bmatrix}\n",
    "    [ \\theta^{(l)}_1 ]^T \\\\\n",
    "    [ \\theta^{(l)}_2 ]^T \\\\\n",
    "    \\vdots \\\\\n",
    "    [ \\theta^{(l)}_{s_{l}} ]^T\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{1,0} & \\dots & \\theta_{1,j} & \\dots  & \\theta_{1,s_{l-1}} \\\\\n",
    "    \\vdots       &       & \\vdots       &        & \\vdots \\\\\n",
    "    \\theta_{i,0} & \\dots & \\theta_{i,j} & \\dots  & \\theta_{i,s_{l-1}} \\\\\n",
    "    \\vdots       &       & \\vdots       &        & \\vdots \\\\\n",
    "    \\theta_{s_l,0} & \\dots & \\theta_{s_l,j} & \\dots  & \\theta_{s_l,s_{l-1}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence, we have the following relation: \n",
    "$$a^{(l)} = g(\\theta^{(l)}.a^{(l-1)})$$\n",
    "\n",
    "\n",
    "## The cost function of a Neural Network\n",
    "\n",
    "The training set is defined by: $ { (x^1,y^1), ..., (x^m,y^m) } $\n",
    "\n",
    "x and y are vectors, with respectively the same dimensions as the input and output layers of the neural network.  \n",
    "\n",
    "The cost function of a neural network is the following:\n",
    "\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[ cost( a^{(L)}_k, y^{(i)}_k) \\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{j=1}^{s_l} \\sum_{i=1}^{s_{l+1}} ( \\theta_{i,j}^{(l)})^2\n",
    "$$\n",
    "\n",
    "$a^{(L)}_k$ is the output of the neural network, and is dependent of the weights ùúÉ of the neural network.  \n",
    "\n",
    "Now, the objective is to train the neural network and find the minimum of the cost function J(ùúÉ).\n",
    "\n",
    "## Mathematic reminder: the chain rule\n",
    "\n",
    "Let‚Äôs define the functions f, g and h as following:\n",
    "\n",
    "$$ f:\\mathbb{R}^n \\rightarrow \\mathbb{R}  $$\n",
    "\n",
    "$$ g:\\mathbb{R}^p \\rightarrow \\mathbb{R}^n $$\n",
    "\n",
    "$$ h = f \\circ g $$\n",
    "\n",
    "The derivative of h is given by the chain rule theorem:\n",
    "\n",
    "$$\n",
    "\\forall_i \\in \\{ \\!1, ... , \\!p \\}, \n",
    "\\frac{\\partial h}{\\partial x_i} = \n",
    "\\sum_{k=1}^{n} \\frac{\\partial f}{\\partial g_k} \\frac{\\partial g_k}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "(See the following [course online](https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/) on partial derivation from the MIT)\n",
    "\n",
    "\n",
    "## The backpropagation algorithm\n",
    "\n",
    "We use the __gradient descent__ to find the minimum of J on ùúÉ: $ \\min\\limits_{\\theta} J(\\theta)$\n",
    "\n",
    "The gradient descent requires to compute: \n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta^{(l)}_{i,j}} $$\n",
    "\n",
    "___In the following parts, we consider only the first part of J(Œ∏) (as if the regularisation term Œª=0). The partial derivative of the second term of J(Œ∏) is easy to compute.___\n",
    "\n",
    "\n",
    "### Definition of ·∫ü\n",
    "\n",
    "Let‚Äôs define the function ·∫ü. When ·∫ü of the layer l is multiplied by the output of the layer (l-1), we obtain the partial derivative of the cost function on Œ∏.\n",
    "\n",
    "Let‚Äôs use the chain rule and develop this derivative on z:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta^{(l)}_{i,j}} \n",
    "=\n",
    "\\sum^{s_l}_{k = 0} \\frac{\\partial J(\\theta)}{\\partial z^{(l)}_k} \\frac{\\partial z^{(l)}_k}{\\partial \\theta^{(l)}_{i,j}}\n",
    "$$\n",
    "\n",
    "(Remind that J is dependent of z)\n",
    "\n",
    "As: \n",
    "$$z^{(l)}_k = [ \\theta^{(l)}_k ]^T . a^{(l-1)} = \\sum_{p=1}^{s_l} \\theta^{(l)}_{k,p} \\times a^{(l-1)}_p$$\n",
    "\n",
    "$$\\frac{\\partial z^{(l)}_k}{\\partial \\theta^{(l)}_{i,j}} = 0\\ for\\ k\\ ‚â†\\ i\\ and\\ p\\ ‚â†\\ j\\ in\\ the\\ sum.$$\n",
    "\n",
    "$$And\\ \\frac{\\partial z^{(l)}_k}{\\partial \\theta^{(l)}_{i,j}} = a^{(l-1)}_j\\ for\\ k\\ =\\ i\\ and\\ p\\ =\\ j\\ in\\ the\\  sum.$$\n",
    "\n",
    "We define the __output error ùõø__:\n",
    "\n",
    "$$ \\delta^{(l)}_k = \\frac{\\partial J(\\theta)}{\\partial z^{(l)}_k} $$\n",
    "\n",
    "So we have:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta^{(l)}_{i,j}} \n",
    "=\n",
    "\\delta^{(l)}_i . a^{(l-1)}_j\n",
    "$$\n",
    "\n",
    "### Value of ·∫ü for the layer L\n",
    "\n",
    "Now let‚Äôs find ùõø for the output layer (layer L):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
